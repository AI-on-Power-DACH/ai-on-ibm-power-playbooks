- name: System setup
  hosts: techzone
  tasks:
    - name: Ping my host
      ansible.builtin.ping:

    - name: Create working directory
      ansible.builtin.file:
        path: "{{ working_directory }}"
        state: directory
        owner: "{{ ansible_user }}"
        mode: 0775
        recurse: true
        follow: false

    - name: Get RHEL version
      ansible.builtin.shell: rpm -E %rhel
      register: rhel_version

    - name: Print RHEL version
      ansible.builtin.debug:
        msg: "RHEL version: {{ rhel_version.stdout }}"

    - name: Add RPM Key
      ansible.builtin.rpm_key:
        state: present
        key: "https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-{{ rhel_version.stdout }}"
      become: true
      become_user: root

    - name: Install enterprise package
      become: true
      become_user: root
      ansible.builtin.dnf:
        name:
          - dnf-plugins-core
          - "https://dl.fedoraproject.org/pub/epel/epel-release-latest-{{ rhel_version.stdout }}.noarch.rpm"
        state: latest

    - name: Enable powertools/codeready builder for RHEL (powertools under CentOS)
      become: true
      become_user: root
      ansible.builtin.command:
        cmd: subscription-manager repos --enable codeready-builder-for-rhel-{{ rhel_version.stdout }}-ppc64le-rpms

    - name: Install the 'Development tools' package group
      become: true
      become_user: root
      ansible.builtin.dnf:
        name: '@Development tools'
        state: present

    - name: Install further system dependencies
      become: true
      become_user: root
      ansible.builtin.dnf:
        name:
          - bzip2
          - cmake
          - curl
          - git
          - openblas-devel
          - libcurl-devel
          - libxcrypt-compat
        state: latest

    - name: Install gcc-toolset (RHEL < 10)
      become: true
      become_user: root
      ansible.builtin.dnf:
        name:
          - gcc-toolset-13
        state: latest
      when: rhel_version.stdout | float < 10

    - name: Install gcc (RHEL >= 10)
      become: true
      become_user: root
      ansible.builtin.dnf:
        name:
          - gcc
        state: latest
      when: rhel_version.stdout | float >= 10

- name: Micromamba setup
  hosts: techzone
  tasks:
    # skip the entire Micromamba setup play if pip should be used.
    - name: Check Mamba vs pip
      ansible.builtin.meta: end_play
      when: use_pip | default(false)

    - name: "Check if micromamba already exists in {{ micromamba_location }}"
      ansible.builtin.stat:
        path: "{{ micromamba_location }}"
      register: dest_stat

    - name: Install micromamba
      ansible.builtin.import_tasks: "{{ base_dir }}/support/download-and-extract-micromamba.yml"
      when: not dest_stat.stat.exists

    - name: Create llm environment
      ansible.builtin.command:
        argv:
          - micromamba
          - create
          - --yes
          - --name=llm
          - --channel=rocketce
          - --channel=defaults
          - "python={{ python_version }}"

    - name: Install basic Python dependencies
      ansible.builtin.command:
        argv:
          - micromamba
          - install
          - --yes
          - --name=llm
          - --channel=rocketce
          - --channel=defaults
          - "python={{ python_version }}"
          - numpy
          - pytorch-cpu
          - sentencepiece
          - "conda-forge::gguf"
  vars:
    arch: linux-ppc64le
    version: latest
    base_dir: "."

- name: Venv setup
  ansible.builtin.import_playbook: "{{ base_dir }}/support/setup-python-venv.yml"
  vars:
    base_dir: .
  when: use_pip | default(false)

- name: LLM setup
  hosts: techzone
  tasks:
    - name: Populate service facts
      ansible.builtin.service_facts:

    - name: Set Venv python facts
      ansible.builtin.set_fact:
        python_command: "{{ venv_python }}"
        python_bin: "{{ venv_bin }}/"
      when: use_pip | default(false)

    - name: Set Micromamba python facts
      ansible.builtin.set_fact:
        python_command: "{{ conda_dir }}/envs/llm/bin/python"
        python_bin: "{{ conda_dir }}/envs/llm/bin/"
      when: not use_pip | default(false)

    - name: Install venv dependencies
      when: use_pip | default(false)
      ansible.builtin.command:
        cmd: '{{ python_command }} -m pip install numpy torch sentencepiece gguf --prefer-binary --extra-index-url "https://wheels.developerfirst.ibm.com/ppc64le/linux https://repo.fury.io/mgiessing"'
      changed_when: false

    - name: Stop already existing llama.cpp service
      ansible.builtin.systemd_service:
        state: stopped
        name: llama.cpp
      become: true
      become_user: root
      when: "'llama.cpp.service' in services"

    - name: Fetch latest version
      ansible.builtin.shell:
        cmd: curl https://api.github.com/repos/ggml-org/llama.cpp/releases/latest -s | jq .name -r
      register: llamacpp_latest_version
      when: llamacpp_version is undefined

    - name: Set clone version
      when: llamacpp_version is undefined
      ansible.builtin.set_fact:
        llamacpp_version: "{{ llamacpp_latest_version.stdout | trim | default('master')}}"

    - name: Print Version
      ansible.builtin.debug:
        msg: "Using llama.cpp Version '{{ llamacpp_version }}'"

    - name: Clone llama.cpp repository
      ansible.builtin.git:
        repo: https://github.com/ggerganov/llama.cpp.git
        dest: "{{ working_directory }}/llama.cpp"
        clone: true
        single_branch: true
        version: "{{ llamacpp_version | default('master') }}"
        force: true
        update: true

    - name: Create build directory
      ansible.builtin.file:
        path: "{{ working_directory }}/llama.cpp/build"
        state: directory
        owner: "{{ ansible_user }}"
        mode: 0775
        recurse: true

    - name: Build llama.cpp with optimizations
      ansible.builtin.shell: |
        cmake -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS ..;
        cmake --build . --config Release -j $(nproc);
      args:
        chdir: "{{ working_directory }}/llama.cpp/build"
      environment:
        PATH: "/opt/rh/gcc-toolset-13/root/usr/bin:{{ ansible_env.PATH }}"

    - name: Make server binary executable
      ansible.builtin.file:
        path: "{{ working_directory }}/llama.cpp/build/bin/llama-server"
        owner: "{{ ansible_user }}"
        mode: 0777

    - name: Install huggingface-cli
      ansible.builtin.shell: |
        {{ python_command }} -m pip install -U "huggingface_hub[cli,hf_xet]"

    - name: Populate model path
      ansible.builtin.set_fact:
        model_path: "{{ working_directory }}/models/{{ model_repository }}"

    - name: Check if model file already exists
      ansible.builtin.stat:
        path: "{{ model_path }}"
      register: model_result

    - name: Download LLM
      ansible.builtin.shell: |
        {{ python_bin }}/hf download \
          {{ model_repository }} {{ model_file }} \
          --local-dir {{ model_path }}
      when: not model_result.stat.exists

    - name: Build parameter list (-)
      ansible.builtin.set_fact:
        llama_cpp_args: >-
          {% set result = [] -%}
          {% for key in llama_cpp_args.keys() -%}
            {% set ignored = result.extend(["-" + key, llama_cpp_args[key] or ""]) -%}
          {%- endfor %}
          {{ result | join(" ") }}
      when: llama_cpp_args is defined

    - name: Build parameter list (--)
      ansible.builtin.set_fact:
        llama_cpp_argv: >-
          {% set result = [] -%}
          {% for key in llama_cpp_argv.keys() -%}
            {% set ignored = result.extend(["--" + key, llama_cpp_argv[key] or ""]) -%}
          {%- endfor %}
          {{ result | join(" ") }}
      when: llama_cpp_argv is defined

    - name: Default llama.cpp parameter list (-)
      ansible.builtin.set_fact:
        llama_cpp_args: ""
      when: llama_cpp_args is not defined

    - name: Default llama.cpp parameter list (--)
      ansible.builtin.set_fact:
        llama_cpp_argv: ""
      when: llama_cpp_argv is not defined

    - name: Print parameter lists
      ansible.builtin.debug:
        msg: "Parameters: {{ llama_cpp_args }} {{ llama_cpp_argv }}"

    - name: Create Systemd Service for llama.cpp
      become: true
      become_user: root
      ansible.builtin.blockinfile:
        create: true
        mode: u=rw,g=r,o=r
        owner: "{{ ansible_user }}"
        path: /etc/systemd/system/llama.cpp.service
        block: |
          [Unit]
          Description=Llama.cpp Service

          [Service]
          Type=simple
          ExecStart={{ working_directory }}/llama.cpp/build/bin/llama-server -m {{ working_directory }}/models/{{ model_repository }}/{{ model_file }} {{ llama_cpp_args }} {{ llama_cpp_argv }}
          User={{ ansible_user }}

          [Install]
          WantedBy=multi-user.target


    # - name: Copy systemd service template for llama.cpp
    #   ansible.builtin.copy:
    #     src: support/template.service
    #     dest: /etc/systemd/system/llama.cpp.service
    #     owner: "{{ ansible_user }}"
    #     mode: u=rw,g=r,o=rwx
    #   become: true
    #   become_user: root

    # - name: Insert service name into llama.cpp service file
    #   ansible.builtin.lineinfile:
    #     path: /etc/systemd/system/llama.cpp.service
    #     regexp: "^Description=(.*)$"
    #     line: "Description=Llama.cpp Service"
    #     backrefs: true
    #   become: true
    #   become_user: root

    # - name: Insert entrypoint into llama.cpp service file
    #   ansible.builtin.lineinfile:
    #     path: /etc/systemd/system/llama.cpp.service
    #     regexp: "^ExecStart=(.*)$"
    #     line: "ExecStart=y
    #     backrefs: true
    #   become: true
    #   become_user: root

    # - name: Insert user into llama.cpp service file
    #   ansible.builtin.lineinfile:
    #     path: /etc/systemd/system/llama.cpp.service
    #     regexp: "^User=(.*)$"
    #     line: "User={{ ansible_user }}"
    #     backrefs: true
    #   become: true
    #   become_user: root

    - name: Start llama.cpp service
      ansible.builtin.systemd_service:
        state: started
        daemon_reload: true
        name: llama.cpp
      become: true
      become_user: root
